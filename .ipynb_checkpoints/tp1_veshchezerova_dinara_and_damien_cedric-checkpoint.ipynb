{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First order methods for regression models\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- gradient descent (GD)\n",
    "- accelerated gradient descent (AGD)\n",
    "- coordinate gradient descent (CD)\n",
    "- stochastic gradient descent (SGD)\n",
    "- stochastic variance reduced gradient descent (SVRG)\n",
    "\n",
    "\n",
    "for the linear regression and logistic regression models, with the \n",
    "ridge penalization.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "## To generate the name of your file, use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp1_veshchezerova_dinara_and_damien_cedric.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using your first and last names\n",
    "fn1 = \"dinara\"\n",
    "ln1 = \"veshchezerova\"\n",
    "fn2 = \"cedric\"\n",
    "ln2 = \"damien\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp1\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "[1. Introduction](#intro)<br>\n",
    "[2. Models gradients and losses](#models)<br>\n",
    "[3. Solvers](#solvers)<br>\n",
    "[4. Comparison of all algorithms](#comparison)<br>\n",
    "\n",
    "<a id='intro'></a>\n",
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Getting model weights\n",
    "\n",
    "We'll start by generating sparse vectors and simulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Simulation of a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(w0, n_samples=1000, corr=0.5, std=0.5):\n",
    "    \"\"\"Simulation of a linear regression model with Gaussian features\n",
    "    and a Toeplitz covariance, with Gaussian noise.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0 : `numpy.array`, shape=(n_features,)\n",
    "        Model weights\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "    \n",
    "    std : `float`, default=0.5\n",
    "        Standard deviation of the noise\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It contains samples of a centered \n",
    "        Gaussian  vector with Toeplitz covariance.\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    n_features = w0.shape[0]\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # Simulation of the labels\n",
    "    y = X.dot(w0) + std * randn(n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x94c6940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEgCAYAAACTnoXDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XucXHV9//HXG0i4SCCUJAZJQtDw\nQxCE5gKixdKCFqm/TQNVobYlLTaxorQ+NJZfpSUi9mcbennwg2pQKNqiSCnBpdCiqBQvqJtELgkY\nDRSbNcQs4WKAQAj5/P44Z81kc2Z3ZvfMnHNm3s/HYx87c+Y7Zz5nZvd85ns9igjMzMzGaq+iAzAz\ns87ghGJmZrlwQjEzs1w4oZiZWS6cUMzMLBdOKGZmlgsnlAqSdJqkkLSw6Fgsf5Iek3R30XGMpCx/\nh62OI9339a3Yd6dxQimRmn+MDxcdi1m3kDRR0lJJpxUdS9XtU3QANir3APsDLxUdiLXE0UAVZhx3\nyt/hRODS9PbdGY/vD7zctmgqzAmlgiJiJ/BC0XE0StL+wEsRsaOJ5+wN7BsRz7cusvK8bq2IeLGo\n125G1f4ORysiOv4Y8+ImrwrKajOu3SbpDyStlfSipJ9I+kid/cyVtELSE2nZdZI+KmmfIeVOknS9\npB9Jel7SVknflrQgY5/Xp3FMlnSdpJ8BzwHThjmehelzzpD0F5IeITlRvbPZWNOy50i6X9ILkv5H\n0qXpvoe+Z7m9rqTXSfpXST9Ny22S9A1Jv1lTZr+0aWVd+j4+LelBScuG7CuzD0XSb6Xv+7Ppz7cl\nzc8o95ikuyW9VtLt6ef1jKSbJU2t9zmM4nhG+jt8X3qsL6TH+ZtpmeMl/aekn0vaIulKSeMafA8a\n6i+RtFf6Od2Txr49/Vv4lKRDa/cH/Hd699J03yHpsZoymX0okt4jabWkben7+xVJv5JRLtL/i1Mk\n/Zek59K/p89KOnC446ga11A6z3uBVwLXAk8Dvwv8taT+iPjCYCFJZwErgPXA3wJPAqcAlwEnAu+o\n2ecC4LXATcBPgEOB84FbJL27dr81vgpsAj4OvAJ4toHYrwDGAZ8Bfg6sazZWSe8Cvgg8AnwM2JHG\n+r9b9brpCerr6b4+TfIeTQLmAicDt6ePXQ38IfB54O+BvYGjgF8f6Y2R9L70+T8ELidpElsI3Cpp\ncURcM+Qph5M036wAlgAnAIuBg4C3jvBajR7PcC4EDgE+S5KkL0pjfQfJ+/xF4NY0lg8Am9Pjyst4\nkuP+N+DLJF9q5gEXAL8iaU5EbAceBj5I8nmsAG5Jnz/s36ukvwY+Anwf+HNgArAI+Iak+RFxx5Cn\nnAj8O/BPwBeA09JYdqbP6wwR4Z+S/JD8kQXw4QbLLczYthGYWLP9AGAAuLdm234kJ/t7gH2G7PuD\n6X5Oq9n2iowYDiA58T40ZPv16fP/pYnjXpg+Zx1wwJDHGo6V5AvST4GfAYfUlDsQeDTjPcvrdXvS\n++8c4TifBO5o4P14DLi75v4hJCe49cBBNdsPIkmcW4d85o9lxUOSkAJ47Qiv3+jxDPd3+FPg4Jrt\nr0+37wTOHrKfVcDjw70HDb5m7TYB+2c8/4KhxwbMTLctrXOcAVxfc//o9Di+BYyv2f4qki9xjwF7\nD3n+TuANQ/Z7O0n/04GN/q+U/cdNXp3nnyLi6cE7kfQFfJfkm/Cgt5DUYv4JmChp0uAPMPjN6q01\n+3hu8LakA9JvsAeQfIs9RtJBGXFcMYrYPxV79l00E+sckn/q6yPiqZr4nyX5pt2q130m/f22Ou8F\nNeVeJ+m4YcpkeQtJLe/KiPj54Mb09v8jSZhnDHnOxoi4aci2wVrHrBFer9HjGc71ETG4HyLiAZLa\n38aIuGVI2W8BU/Ns/onENkj6xZSM5JrErvfg5DHsfj5JwvqbSGo5g6+5keQL1RHALw95zr0R8d0h\n275O8iVo5hhiKRU3eXWeRzO2bSFpphp0TPr7umH288rBG5KmkDRHzAemZJSdSHKyqPWjESPdU9Zz\nmon1yPT3uowyWdtyed2I+C9Jnyep8bxbUh9wF/CliHiopvyfAv8MPCjpUeAbwG3AbZF0cNczeFxr\nMx5bk/5+9ZDt9f4OYPe/hT00cTzDyXr9p4ANdbYPxtVI02hDJL0T+BDJyX3ckIcPGcOuG/08VtZs\nH/XnUSVOKJ2nkeGNSn8vAe6rU2YjgCQBXyE5wV4J9JF8g30Z+APgd8gY3JHxjb8RWc9pONaasu1+\nXSLi/LRz/SzgV0hOZB+V9KcRcVVa5suSZqZlfpWkVnEB8E1JZ9R+260TSzOG+zsYcX+NHM8oX7/R\nuOoNm27onCXpbOBLJH0cf0KSyF4g6bf6T8Y2IKntn0dVOKF0px+nv5+LiLtGKPt6kg7dyyLi0toH\nJL2nFcEN0Uysg6N1js54LGtbXq8LQESsIfmG+jeSJgLfAz4p6eoYbEyPeBL4F+Bf0mT9SZLO3fnA\nv9bZ9SPp79cBXxvy2LHp76xvwGPSyPG00JPAL2VsH1oTq+f3SBLIr9V+uZH02oyyzR5L7efxyJDH\nWvZ5VIH7ULrTnSSjai6WtMc/raT9JU1I7w5+s9KQMseRjP5qtWZiXQk8DiyUdEhNmQNJRr+15HUl\n/ZKk3f6X0n6s/ybpa9pvsB1/SJkAfpDezTp5DvoqySilD9QcK+ntD5A0E321yeOrq5Hjyeu1hvEj\n4LWSDq+Ja1+S0WONeJkkUfziONIEfklG2cFmtuE+g1q96b6X1A53lnQYSa39J+z6XLuKayjldLqk\nrH/aJyJiuM7lhkTEc5J+n2TY5jpJ15GMIJpIMjz4bJJkcTfJsMq1wEckDY7s+l8kQ1DXALPHGk9e\nsUbEDiXL1twAfF/StSTDhheStFcfSYPfRpt8j34f+KCkwSHGL5E0af0GcFNEbEuTyeOSeklONpvT\neP6YpA/htmFieVrJXKKrge/VzIlYSNLBvri2AzwHIx5Pjq9Vz1XAucBdkj5NMgz498hunsxyM3AO\n8PW0P2gc8FskCXE3EbFF0nrgXCVzkX5GUjPN/EwiYl3aHPgR4B5JX2LXsOEDgXdHRFfOrHdCKacz\n05+h1jH8aKWGRcSdkuYBF5PMVZlMcmJ7BPg74IG03MtKJqRdQTKf4xUkieR8kqawliaUZmJNy35B\n0g6Sb6IfIzk5XJuWuQVo+GTYxOveTdLx+3bgMJJvx/8NfJjkxAjJifAfgNNJ+k4OJKlN9QL/Nx0h\nNFws/yjpcZI+ncGmx/uBBRFxa6PH1KC7Gfl4Wioivq1k8uKfA8tIhiF/iqQWOrTZL+v5N6Y1uA+S\n/O0OJu2L2dUZXuvdJHNR/ook6fyE4ZP8n6VJ6H0kzZbbSZoEfycivtnYUXYetb4p1Kx4kj5EcmI5\nJWP4ppnlwAnFOoqk8cDLtU0OaR/KAyQTAV81zGgqMxsDN3lZp3k18B+SbiRppjmMpHnuSOCPnUzM\nWscJxTrNAMnKAO8mmYS5A3gQuDhj5riZ5chNXmZmlgvPQzEzs1x0VZPXpEmTYubMmUWHYWZWGatW\nrXoiIiY3UrarEsrMmTNZuXLlyAXNzAwAST9ptKybvMzMLBdOKGZmlgsnFDMzy0VX9aGYmY3GSy+9\nRH9/Py+88ELRobTMfvvtx7Rp0xg3bui1yBrnhGJmNoL+/n4mTJjAzJkzSVbB7ywRwZYtW+jv7+fI\nI48c+Ql1uMnLzGwEL7zwAoceemhHJhMASRx66KFjroE5oZiZNaBTk8mgPI7PCcXMOlpf73I2LZ3F\nzksPZtPSWfT1Li86pDFbunQpV1xxRd3Hb731Vh566KE2RpRwQjGzjtXXu5zjVl3CVAbYSzCVAY5b\ndUlHJJXhOKGYmeVs+upl7K/dr1iwv7YzffWy1r7wAzfB3x8HSycmvx8Y+0LXn/jEJzj66KM544wz\nWLduHQCf+cxnmDdvHieccALnnHMOzz//PN/5znfo7e1lyZIlnHjiiTzyyCOZ5VrBCcXMOtaUGKiz\n/YnWvegDN8FtF8EzG4BIft920ZiSyqpVq7jxxhv5wQ9+wC233EJfXx8AZ599Nn19fdx///0cc8wx\nXHvttbzxjW+kp6eHZcuWcd999/Ga17wms1wrOKGYWcfarOw1DTdrUute9GuXwUvbdt/20rZk+yh9\n85vfZMGCBRxwwAEcdNBB9PT0ALBmzRpOPfVUjj/+eG644QbWrl2b+fxGy42VE4qZdawNs5ewLcbv\ntm1bjGfD7CWte9Fn+pvb3qCsUVgLFy7kqquu4sEHH+TSSy+tO+y30XJj5YRiZh1rXs9i1sy5nE1M\nZmeITUxmzZzLmdezuHUvevC05rY34M1vfjMrVqxg27ZtbN26ldtuuw2ArVu3cthhh/HSSy9xww03\n/KL8hAkT2Lp16y/u1yuXN8+UN7OONq9nMaQJZGr601Kn/2XSZ1Lb7DVu/2T7KM2ePZt3vetdnHji\niRxxxBGceuqpAHz84x/n5JNP5ogjjuD444//RRI599xz+aM/+iOuvPJKbr755rrl8tZVlwCeO3du\n+HooZtashx9+mGOOOabxJzxwU9Jn8kx/UjM5/S/h9e9sXYA5yTpOSasiYm4jz3cNxcwsb69/ZyUS\nSN5K2Yci6TpJmyWtqfP4aZKekXRf+jP6uqSZmeWirDWU64GrgM8PU+abEfH29oRjZmYjKWVCiYh7\nJM0sOg4za15f73Kmr17GlBhgsyazYfaS1o6qapOI6OgFIvPoTy9lQmnQKZLuBzYCH46I1szUMbOG\nDa6dtb+2Q7p21sGrLqEPKp1U9ttvP7Zs2ZLLEvbPPT3A+Oc3sU/sYIf2YfsBU3nFxOwJmO0yeD2U\n/fbbb0z7qWpCWQ0cERHPSjoLuBU4KqugpEXAIoAZM2a0L0KzLjTs2lkVTijTpk2jv7+fgYHspVwa\n9eLzWxm//WnErtpA8Djbx09k3wMmjDXMMRm8YuNYVDKhRMTPa27fIekfJU2K2HOBnoi4BrgGkmHD\nbQzTrOtMiQHI+ALf0rWz2mDcuHFjupLhoE1LZzGVPZPSJiYzden6Me+/aKUc5TUSSVOV1jslnURy\nHFuKjcrMClk7q0IKWayyjUqZUCR9EbgXOFpSv6QLJL1X0nvTIr8NrEn7UK4Ezo1umqFpVlKFrJ1V\nIZ2ecEvZ5BUR543w+FUkw4rNrETm9SymD9JRXk+wWZPYMKczRnnlYcPsJRw8OGghtS3Gs2HOktYv\nCdMGXnrFzKyNdg2rThNuyYdVN7P0ihOKmZnV1UxCKWUfipmZVY8TipmZ5cIJxczMcuGEYmZmuXBC\nMTOzXDihmJlZLpxQzMwsF04oZmaWi1IuvWJmZqNT5AXOnFDMzDpE0Rc4c5OXmVmHGPYCZ23ghGJm\n1iGKvt6KE4qZWQ76epezaeksdl56MJuWzqKvd3nbYyj6eitOKGZmYzTYdzGVAfZK+y6OW3VJ25NK\n0Rc4c0IxMxujovsuBs3rWcyaOZezicnsDLGJyayZc7lHeZmZVcWUGABlbW//teLn9SyGNIFMTX/a\nxTUUszEqQ9u5FavovouycEIxG4OytJ1bsYruuyiLUiYUSddJ2ixpTZ3HJelKSeslPSBpdrtjNIPy\ntJ1bsYruuyiLsvahXA9cBXy+zuNvA45Kf04GPpX+NmurMrWdW7GK7Lsoi1LWUCLiHuDJYYrMBz4f\nie8CEyUd1p7ozHZx27nZLqVMKA04HNhQc78/3WYlVrbO6zzicdt5+T5XK05Zm7xGktHIQGQWlBYB\niwBmzJjRyphsGEUvWteqeOb1LKYP0tVdn2CzJrFhTvtWdy1a2T5XK5YiMs/DhZM0E/j3iDgu47Hl\nwN0R8cX0/jrgtIh4fLh9zp07N1auXNmCaG0km5bOYip7rjO0iclMXbq+6+Mpk2aWP/f72PkkrYqI\nuY2UrWqTVy/w++lorzcAz4yUTKxYRS9at+frliuesmh2GLTfR6tVyoQi6YvAvcDRkvolXSDpvZLe\nmxa5A3gUWA98BnhfQaFag8rWeV22eMqi2WHQfh+tVikTSkScFxGHRcS4iJgWEddGxKcj4tPp4xER\nF0bEayLi+IhwO1bJla3zumzxlEWzNQ6/j1arlAnFOk/ZJn61M54qjYJqtsZRts/VilXaTvlWcKe8\ntdtuo6BS22J8aU+6VYvXWq8bOuXNKqFqS7O4xmFjUdV5KGaVUMWlWRpdQqSZ4cXWHVxDMWuhTh0F\n1cmrLFepz6tsXEMxa6ENs5dwcEafxIY5Syq9eOCwTXkVqaVk1bAAz/wfAycUsxbq1KVZqtiUV6ve\nkjEvaHzlE2WRnFDMWqwTlzXfrMmZS65s1qRKHF+9GtZ+sb3SibJo7kMxs6ZVfUJjvQmc9VS9z6td\nnFDMrGlVH15cb7DE05pQ6URZNDd5mdmoVLkpr95gifVz/gLovD6vdnFCMbOuM+JgiYomyqJ56RUz\nM6vLS6+YmVnbOaGYmVkunFDMzCwXTihmZpYLj/Iys914FWEbLScUM/uFemtceXFEa4SbvMya0Mql\nzcuwbHrVLgjWqDK8t92glAlF0pmS1klaL+nijMcXShqQdF/6854i4rTu0sprgJTl+iL11riq8uKI\nWe/tnFUf4d4rFxYdWscpXUKRtDdwNfA24FjgPEnHZhT9UkScmP58tq1BWldq5bf3stQMOvGCYFnv\n7V6Ck7escE0lZ6VLKMBJwPqIeDQitgM3AvMLjsmspd/e6+37lTHQ1maaqq8inKXee7uXqHxTXtmU\nMaEcDmyoud+fbhvqHEkPSLpZ0vT2hGbdrJXf3uvtW6KtTWBVX0U4S733FqrdlFdGZUwoGZe3YeiC\nY7cBMyPi9cBdwOfq7kxaJGmlpJUDA81dA8GsViu/vWfte6h2NYHN61nM1KXr2etjTzN16fpKJxNI\n3tuddZYsrHJTXhmVMaH0A7U1jmnAxtoCEbElIl5M734GmFNvZxFxTUTMjYi5kyfX/6ZiNpJWfnsf\nuu96a7b6G3Xz5vUs5nuHLtgjqVS9Ka+MSrfasKR9gB8BpwM/BfqA34mItTVlDouIx9PbC4A/i4g3\njLRvrzbcmVoxEa/oyX2bls7KvMTuJiYzden6tsXRSXZ9puly9Z6w2ZBmVhsu3cTGiNgh6f3AncDe\nwHURsVbSZcDKiOgFLpLUA+wAngQWFhawFaoVE/HKMLmv3gWgNsxZ4utzjFKVLwhWFaWrobSSayid\npxXf5MtSOyjLN+qia2tWrErXUMzqyTqxzYmBzGEcU2KATUtnjeokOKXuPtvbf1GGb9RlqK1ZdTih\nWCXUO7E9owM5hGcznzOVgVGdBDdrcmYNZbMm7XZS74Zv7sNNuNx1Cd3OPX5rThlHeZntod6JDbTH\ncNudkczdGFq20SG3jQwPLstSKa023ITLbjh+a44TilVCvRPbwfHsHkN5syYyJftorMmqkeHBjS6V\nUvVFCetNCnyZvUqxVIyVi5u8rBKGa4Ya2tdQr1N9aJPVcEbqv2ikn6UT+h/qjTbbl+2Z5T1Ppru5\nhmKV0Mws9XasR9XIMixlWfBxLOrV1jpxEUkbO9dQrBLm9Syu6QROh9HOye4EbqbsaDUyT6Qso8XG\nKqu21geeJ2N78DwUs1EaaZ5IWeaztEpZ5slYazUzD8UJxYDuGAKbpZXHvVsfSmpbjK/86r3WXTyx\n0ZrSCZ3Ho9Hq425H05tZmbiGYpVrmsmrVlG14zYrgmso1pQqdR7nWauo0nG3Qrc2c1rreNiwVWoI\naJ5Dcat03Hnrlpn+1l5OKFap64jneV33Kh133jphjoyVjxOKVeo64nnWKkZ73FVfTgXyTcxmg9yH\nYkA5lkpvRN4XnmrkuGv7Gp7RgZwQLzBeOyo9Iq7RFZXNmuEailVKu2tTQ/saDuHZJJnUqGJTUTc3\n91nruIZildPO2lRWX0OWqjUVDTdHxqO/bLSaSiiSvhYRp0u6BFgFrI6In7UmNLPi1RtaPFQVm4oy\n1+jq0kmulo9mm7zOSX+PAy4E7pPUL+nLki7NNzSz4tUbBFCrk5qKPPrLxmLEhCLp3MHbEfF0+vvS\niHh7RBwGnAxc28i+GiXpTEnrJK2XdHHG4/tK+lL6+Pckzczrtc1qZfU1vBh78xQTSj8ibjQ8+svG\nopEmr89LWgS8PyIeGvpgRPwU+CnQm0dAkvYGrgbeAvQDfZJ6h7z2BcBTETErTXh/Dbwrj9c3qzXS\nelxlHhE3Gh79ZWPRSK1iDkkT1w8kXSHpwBbHdBKwPiIejYjtwI3A/CFl5gOfS2/fDJwuqYGWbrPm\nzetZzNSl69nrY08zden6jqmNZPHoLxuLERNKRDwYEacCi4DfBdZJOq+FMR0ObKi5359uyywTETuA\nZ4BDWxiTWVeo0iRXK5+GR3lFxOck3Qr8FfDPNc1ga3OOKaumMXRJ5EbKJAWTOBcBzJgxY2yRmXWB\nqkxytfJpqiM9Ip6JiAuBecAkkmawv5U0IceY+oHpNfenARvrlZG0D3Aw8GSdmK+JiLkRMXfy5JFH\n7JiZ2eg0lFAkjZN0kqSLJH0B+DfgdSQ1nAuBH0rqySmmPuAoSUdKGg+cy54d/r3A+ent3wa+Ht10\nYRczsxIasclL0neAE4F9gZ3A/cBtwLeBbwHPApcCN0u6KCI+PZaAImKHpPcDdwJ7A9dFxFpJlwEr\nI6KXZJjyP0taT1IzObf+Hq1qPFPbrJpGvGKjpK+QJI5vA9+NiOfqlPsISZ9KaTsqfMXG8vN12M3K\npZkrNjYyyuutEXFZRHytXjJJ3UPS32E2ap6pbVZdea42fD97zhcxa4pnaptVV24JJSK2RcRtee3P\nulM3X5Z3OJ1wUS/rfL4eipWKZ2rvydd/t6pwQrFS8UztPblfyarCF9iy0vFM7d3VuyaL+5WsbFxD\nsVJz34H7law6nFCstNx3kHC/klWFE4qVlvsOEu5XsqpwH4qVlvsOdnG/klWBayhWWu47MKsWJxQr\nLfcdmFWLE4qVlvsOzKplxNWGO4lXGzYza06uqw2bmZk1wqO8zErGFxizqnJCMSuRe69cyMlbVrCX\ngHQy58GrLqEPnFSs9NzkZVYSfb3LdyWTGt04mdOqyQnFrCSmr162RzIZ1I2TOa16nFDMSqLe1SrB\nkzmtGkqVUCT9kqSvSvpx+vuQOuVelnRf+tPb7jjNWqHeygA7A0/mtEooVUIBLga+FhFHAV9L72fZ\nFhEnpj897QvPrHWyVgbYGfC9Qxe4Q94qoWwJZT7wufT254DfKjAWs7bKWhlg1Zy/4ZSLri86NMDX\nprGRlWqmvKSnI2Jizf2nImKPZi9JO4D7gB3AJyPi1kb275nyZqMzeG2a2ssJbIvxXgqnCzQzU77t\n81Ak3UX26tsfbWI3MyJio6RXA1+X9GBEPFLn9RYBiwBmzJjRdLw2Mk/E63zDXpvGn7Wl2p5QIuKM\neo9J+pmkwyLicUmHAZvr7GNj+vtRSXcDvwxkJpSIuAa4BpIayhjDtyF2++bqiXillEfC97VprBFl\n60PpBc5Pb58PfHloAUmHSNo3vT0JeBPwUNsitN34qorlltdllH1tGmtE2RLKJ4G3SPox8Jb0PpLm\nSvpsWuYYYKWk+4FvkPShOKEUpN7cCX9zLYe8Er6vTWONKNVaXhGxBTg9Y/tK4D3p7e8Ax7c5NKvj\nGR3IITy7x/bNmuTL1JZAXk1V83oW0wdp09kTbNYkNsxxX5ntrlQJxaqlr3c5J8QLe5ywXoy92TBn\niRNKCWzWZKayZy1yNAnf17W3kZStycsqZPrqZYzXjj22P68D/M21JNxUZe3kGor9QrOjgeo1pxwc\nezaBWTHcVGXt5IRiwOiG/+bZnGKt46Yqaxc3eRkwutFAbk4xs1pOKAaMbvhv1tpTXorDrHu5ycuA\n0TdfuTnFzAa5hmKAm6/MbOycUAxw85WZjV2plq9vNS9fb2bWnGaWr3cNxczMcuGEYmZmuXBCMTOz\nXDihmJlZLpxQzMwsF04oZmaWCycUMzPLhROKmZnlwgnFzMxy4YRiZma5KFVCkfQOSWsl7ZRUd6q/\npDMlrZO0XtLF7YzRzMyylSqhAGuAs4F76hWQtDdwNfA24FjgPEnHtie8xvX1LmfT0lnsvPRgNi2d\nRV/v8qJDMjNrqVJdDyUiHgaQMi5UvstJwPqIeDQteyMwH3io5QE2aDSX0zUzq7qy1VAacTiwoeZ+\nf7qtNEZzOV0zs6prew1F0l1kX9jvoxHx5UZ2kbGt7hr8khYBiwBmzJjRUIxjNSUGMqMc7nK63aqv\ndznTVy9jSgywWZPZMHuJa3FmFdX2hBIRZ4xxF/3A9Jr704CNw7zeNcA1kFwPZYyv3ZBmL6fbrSdV\nNw2adZYqNnn1AUdJOlLSeOBcoLfgmHbTzOV0B0+qUxlgr/SketyqS7qiE99Ng2adpVQJRdICSf3A\nKcDtku5Mt79K0h0AEbEDeD9wJ/AwcFNErC0q5izNXE63m0+qU2LPWlyy3U2DZlVUtlFeK4AVGds3\nAmfV3L8DuKONoTVtXs9iSBPIVLI7jaC7+1uabRo0s3IrVQ2lG23W5DrbJ7U5kvZrpmnQzMrPCaVg\n3XxSbaZp0MzKTxFtGfhUCnPnzo2VK1cWHcYedo3yeoLNmtQ1o7zMrPwkrYqIukth7VbWCcXMzOpp\nJqG4ycvMzHLhhGJmZrlwQjEzs1w4oZiZWS6cUMzMLBdOKGZmlgsnFDMzy4UTipmZ5cIJxczMcuGE\nYmZmuXBCMTOzXDihmJlZLpxQzMwsF04oZmaWCycUMzPLhROKmZnlolQJRdI7JK2VtFNS3Qu6SHpM\n0oOS7pPkK2aZmZXAPkUHMMQa4GxgeQNlfy0inmhxPGZm1qBSJZSIeBhAUtGhWIfr613O9NXLmBID\nbNZkNsxewryexUWHZVZppWryakIAX5G0StKiooOxaunrXc5xqy5hKgPsJZjKAMetuoS+3kYqxmZW\nT9sTiqS7JK3J+JnfxG7eFBGzgbcBF0p68zCvt0jSSkkrBwYGxhy/Vd/01cvYX9t327a/tjN99bKC\nIjLrDG1v8oqIM3LYx8b092ZJK4CTgHvqlL0GuAZg7ty5MdbXtuqbEgOQ0ao6xV1yZmNSuSYvSa+Q\nNGHwNvBWks58s4Zs1uQ62ycC0y1bAAAE0ElEQVS1ORKzzlKqhCJpgaR+4BTgdkl3pttfJemOtNgr\ngW9Juh/4PnB7RPxnMRFbFW2YvYRtMX63bdtiPBtmLykoIrPOULZRXiuAFRnbNwJnpbcfBU5oc2iA\nRwZ1ink9i+mD9LN8gs2axIY5/izNxkoR3dOtMHfu3Fi5cnTzIAdHBtV25m6L8ayZc7lPRGbWsSSt\nioi6E81rlarJq8w8MsjMbHhOKA2aEtlDjj0yyMws4YTSII8MMjMbnhNKgzwyyMxseE4oDZrXs5g1\ncy5nE5PZGWITk90hb2ZWw6O8zMysLo/yMjOztnNCMTOzXDihmJlZLpxQzMwsF04oZmaWCycUMzPL\nhROKmZnlwgnFzMxy0VUTGyUNAD9p0e4nAd26UqSPvTt167F323EfERHZixkO0VUJpZUkrWx0Nmmn\n8bH72LtJtx53I9zkZWZmuXBCMTOzXDih5OeaogMokI+9O3XrsXfrcY/IfShmZpYL11DMzCwXTihm\nZpYLJ5QcSVom6YeSHpC0QtLEomNqF0nvkLRW0k5JHT+kUtKZktZJWi/p4qLjaSdJ10naLGlN0bG0\nk6Tpkr4h6eH0b/1Pio6pbJxQ8vVV4LiIeD3wI+D/FBxPO60BzgbuKTqQVpO0N3A18DbgWOA8SccW\nG1VbXQ+cWXQQBdgBfCgijgHeAFzYZZ/7iJxQchQRX4mIHend7wLTioynnSLi4YhYV3QcbXISsD4i\nHo2I7cCNwPyCY2qbiLgHeLLoONotIh6PiNXp7a3Aw8DhxUZVLk4orfOHwH8UHYS1xOHAhpr7/fjE\n0lUkzQR+GfhesZGUyz5FB1A1ku4CpmY89NGI+HJa5qMk1eMb2hlbqzVy7F1CGds8/r5LSDoQ+Dfg\nTyPi50XHUyZOKE2KiDOGe1zS+cDbgdOjwyb5jHTsXaQfmF5zfxqwsaBYrI0kjSNJJjdExC1Fx1M2\nbvLKkaQzgT8DeiLi+aLjsZbpA46SdKSk8cC5QG/BMVmLSRJwLfBwRPxd0fGUkRNKvq4CJgBflXSf\npE8XHVC7SFogqR84Bbhd0p1Fx9Qq6cCL9wN3knTM3hQRa4uNqn0kfRG4FzhaUr+kC4qOqU3eBPwe\n8Ovp//d9ks4qOqgy8dIrZmaWC9dQzMwsF04oZmaWCycUMzPLhROKmZnlwgnFzMxy4YRiZma5cEIx\nazNJsyS9JOljQ7Z/StLWblj+3zqTE4pZm0XEeuCzwAclTQKQ9JckC4ouiIiVRcZnNlqe2GhWAElT\ngUeAfwR+CFwDnBcRNxUamNkYeHFIswJExCZJ/wB8iOT/8KLaZCLpEOCWiPi1omI0a5abvMyK82Ng\nX+DeiLi69oGIeMrJxKrGCcWsAJJ+HVhOssjimySdMOTxy9J+FbPKcEIxazNJs4FbSTrmTwP+B/ir\nIcXmAKvaG5nZ2DihmLWRpFkkl4b+CvCB9Jr0HwPOkvTmmqJzgNUFhGg2ah7lZdYm6ciu75DUSH4j\nIl5Mt+8NrAGeiog3SpoGfD8iXlVctGbN8ygvszaJiE3AqzO2vwwcU7PJzV1WSW7yMisfN3dZJbnJ\ny8zMcuEaipmZ5cIJxczMcuGEYmZmuXBCMTOzXDihmJlZLpxQzMwsF04oZmaWCycUMzPLhROKmZnl\n4v8Dw7W8q904kTUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x94c6518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 50\n",
    "w0 = np.array([0.5])\n",
    "\n",
    "X, y = simu_linreg(w0, n_samples=n_samples, corr=0.3, std=0.5)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(r\"$x_i$\", fontsize=16)\n",
    "plt.ylabel(r\"$y_i$\", fontsize=16)\n",
    "plt.title(\"Linear regression simulation\", fontsize=18)\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Simulation of a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function (overflow-proof)\"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size)    \n",
    "    out[idx] = 1 / (1. + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def simu_logreg(w0, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a logistic regression model with Gaussian features\n",
    "    and a Toeplitz covariance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0 : `numpy.array`, shape=(n_features,)\n",
    "        Model weights\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It contains samples of a centered \n",
    "        Gaussian vector with Toeplitz covariance.\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    n_features = w0.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p = sigmoid(X.dot(w0))\n",
    "    y = np.random.binomial(1, p, size=n_samples)\n",
    "    # Put the label in {-1, 1}\n",
    "    y[:] = 2 * y - 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "w0 = np.array([-3, 3.])\n",
    "\n",
    "X, y = simu_logreg(w0, n_samples=n_samples, corr=0.4)\n",
    "\n",
    "plt.scatter(*X[y == 1].T, color='b', s=10, label=r'$y_i=1$')\n",
    "plt.scatter(*X[y == -1].T, color='r', s=10, label=r'$y_i=-1$')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(r\"$x_i^1$\", fontsize=16)\n",
    "plt.ylabel(r\"$x_i^2$\", fontsize=16)\n",
    "plt.title(\"Logistic regression simulation\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='models'></a>\n",
    "# 2. Models gradients and losses\n",
    "\n",
    "We want to minimize a goodness-of-fit function $f$ with ridge regularization, namely\n",
    "$$\n",
    "\\arg\\min_{w \\in \\mathbb R^d} \\Big\\{ f(w) + \\frac{\\lambda}{2} \\|w\\|_2^2 \\Big\\}\n",
    "$$\n",
    "where $d$ is the number of features and where we will assume that $f$ is $L$-smooth.\n",
    "We will consider below the following cases.\n",
    "\n",
    "**Linear regression**, where \n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - x_i^\\top w)^2 + \\frac{\\lambda}{2} \\|w\\|_2^2 = \\frac{1}{2 n} \\| y - X w \\|_2^2 + \\frac{\\lambda}{2} \\|w\\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, $y = [y_1 \\cdots y_n]$ is the vector of labels and $X$ is the matrix of features with lines containing the features vectors $x_i \\in \\mathbb R^d$.\n",
    "\n",
    "**Logistic regression**, where\n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{n} \\sum_{i=1}^n \\log(1 + \\exp(-y_i x_i^\\top w)) + \\frac{\\lambda}{2} \\|w\\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, and where labels $y_i \\in \\{ -1, 1 \\}$ for all $i$.\n",
    "\n",
    "We need to be able to compute $f(w)$ and its gradient $\\nabla f(w)$, in order to solve this problem, as well as $\\nabla f_i(w)$ for stochastic gradient descent methods and $\\frac{\\partial f(w)}{\\partial w_j}$ for coordinate descent.\n",
    "\n",
    "Below is the full implementation for linear regression.\n",
    "\n",
    "## 2.1 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class ModelLinReg:\n",
    "    \"\"\"A class giving first order information for linear regression\n",
    "    with least-squares loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : `numpy.array`, shape=(n_samples, n_features)\n",
    "        The features matrix\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        The vector of labels\n",
    "    \n",
    "    strength : `float`\n",
    "        The strength of ridge penalization\n",
    "    \"\"\"    \n",
    "    def __init__(self, X, y, strength):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.strength = strength\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        \"\"\"Computes f(w)\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return 0.5 * norm(y - X.dot(w)) ** 2 / n_samples + strength * norm(w) ** 2 / 2\n",
    "    \n",
    "    def grad(self, w):\n",
    "        \"\"\"Computes the gradient of f at w\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return X.T.dot(X.dot(w) - y) / n_samples + strength * w\n",
    "\n",
    "    def grad_i(self, i, w):\n",
    "        \"\"\"Computes the gradient of f_i at w\"\"\"\n",
    "        x_i = self.X[i]\n",
    "        return (x_i.dot(w) - y[i]) * x_i + self.strength * w\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        \"\"\"Computes the partial derivative of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return X[:, j].T.dot(X.dot(w) - y) / n_samples + strength * w[j]\n",
    "\n",
    "    def lip(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return norm(X.T.dot(X), 2) / n_samples + self.strength\n",
    "\n",
    "    def lip_coordinates(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return (X ** 2).sum(axis=0) / n_samples + self.strength\n",
    "        \n",
    "    def lip_max(self):\n",
    "        \"\"\"Computes the maximum of the lipschitz constants of f_i\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return ((X ** 2).sum(axis=1) + self.strength).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Checks for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simulation setting\n",
    "n_features = 50\n",
    "nnz = 20\n",
    "idx = np.arange(n_features)\n",
    "w0 = (-1) ** idx * np.exp(-idx / 10.)\n",
    "w0[nnz:] = 0.\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.stem(w0)\n",
    "plt.title(\"Model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "X, y = simu_linreg(w0, corr=0.6)\n",
    "model = ModelLinReg(X, y, strength=1e-3)\n",
    "w = np.random.randn(n_features)\n",
    "\n",
    "print(check_grad(model.loss, model.grad, w)) # This must be a number (of order 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"lip=\", model.lip())\n",
    "print(\"lip_max=\", model.lip_max())\n",
    "print(\"lip_coordinates=\", model.lip_coordinates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logistic regression\n",
    "\n",
    "**NB**: you can skip these questions and go to the solvers implementation, and come back here later.\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Compute (on paper) the gradient $\\nabla f$, the gradient of $\\nabla f_i$ and the gradient of the coordinate function $\\frac{\\partial f(w)}{\\partial w_j}$ of $f$ for logistic regression (fill the class given below).\n",
    "\n",
    "2. Fill in the functions below for the computation of $f$, $\\nabla f$, $\\nabla f_i$ and $\\frac{\\partial f(w)}{\\partial w_j}$ for logistic regression in the ModelLogReg class below (fill between the TODO and END TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLogReg:\n",
    "    \"\"\"A class giving first order information for logistic regression\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : `numpy.array`, shape=(n_samples, n_features)\n",
    "        The features matrix\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        The vector of labels\n",
    "    \n",
    "    strength : `float`\n",
    "        The strength of ridge penalization\n",
    "    \"\"\"    \n",
    "    def __init__(self, X, y, strength):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.strength = strength\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        \"\"\"Computes f(w)\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "        f = 0\n",
    "        for i in range(n_samples):      \n",
    "            f = f + math.log(1 + math.exp(- y[i] * (X[i].dot(w))))\n",
    "        f = f / n + strength * norm (w) ** 2 /2\n",
    "        ### END TODO\n",
    "       \n",
    "    def grad(self, w):\n",
    "        \"\"\"Computes the gradient of f at w\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def grad_i(self, i, w):\n",
    "        \"\"\"Computes the gradient of f_i at w\"\"\"\n",
    "        x_i = self.X[i], strength = self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        \"\"\"Computes the partial derivative of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "            #return \n",
    "        ### END TODO\n",
    "\n",
    "    def lip(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip_coordinates(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip_max(self):\n",
    "        \"\"\"Computes the maximum of the lipschitz constants of f_i\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Checks for the logistic regression model\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Check numerically the gradient using the function ``checkgrad`` from ``scipy.optimize`` (see below), as we did for linear regression above\n",
    "\n",
    "**Remark**: use the function `simu_logreg` to simulate data according to the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='solvers'></a>\n",
    "## 3. Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have classes `ModelLinReg` and `ModelLogReg` that allow to compute $f(w)$, $\\nabla f(w)$, \n",
    "$\\nabla f_i(w)$ and $\\frac{\\partial f(w)}{\\partial w_j}$ for the objective $f$\n",
    "given by linear and logistic regression.\n",
    "\n",
    "We want now to code and compare several solvers to minimize $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tools'></a>\n",
    "## 3.1. Tools for the solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting point of all solvers\n",
    "w0 = np.zeros(model.n_features)\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 50\n",
    "\n",
    "# Random samples indices for the stochastic solvers (sgd, sag, svrg)\n",
    "idx_samples = np.random.randint(0, model.n_samples, model.n_samples * n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspector(model, n_iter, verbose=True):\n",
    "    \"\"\"A closure called to update metrics after each iteration.\n",
    "    Don't even look at it, we'll just use it in the solvers.\"\"\"\n",
    "    objectives = []\n",
    "    it = [0] # This is a hack to be able to modify 'it' inside the closure.\n",
    "    def inspector_cl(w):\n",
    "        obj = model.loss(w)\n",
    "        objectives.append(obj)\n",
    "        if verbose == True:\n",
    "            if it[0] == 0:\n",
    "                print(' | '.join([name.center(8) for name in [\"it\", \"obj\"]]))\n",
    "            if it[0] % (n_iter / 5) == 0:\n",
    "                print(' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8)]))\n",
    "            it[0] += 1\n",
    "    inspector_cl.objectives = objectives\n",
    "    return inspector_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gd'></a>\n",
    "## 3.2 Gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `gd` below that implements the gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Gradient descent\n",
    "    \"\"\"\n",
    "    step = 1 / model.lip()\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    if verbose:\n",
    "        print(\"Lauching GD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_gd = inspector(model, n_iter=n_iter)\n",
    "w_gd = gd(model, w0, n_iter=n_iter, callback=callback_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='agd'></a>\n",
    "## 3.3 Accelerated gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `agd` below that implements the accelerated gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Accelerated gradient descent\n",
    "    \"\"\"\n",
    "    step = 1 / model.lip()\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    # An extra variable is required for acceleration\n",
    "    z = w0.copy()\n",
    "    t = 1.\n",
    "    t_new = 1.    \n",
    "    if verbose:\n",
    "        print(\"Lauching AGD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO        \n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_agd = inspector(model, n_iter=n_iter)\n",
    "w_agd = agd(model, w0, n_iter=n_iter, callback=callback_agd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cgd'></a>\n",
    "\n",
    "## 3.4 Coordinate gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `cgd` below that implements the coordinate gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cgd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Coordinate gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    n_features = model.n_features\n",
    "    steps = 1 / model.lip_coordinates()\n",
    "    if verbose:\n",
    "        print(\"Lauching CGD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_cgd = inspector(model, n_iter=n_iter)\n",
    "w_cgd = cgd(model, w0, n_iter=n_iter, callback=callback_cgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>\n",
    "## 3.5. Stochastic gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "- Finish the function `sgd` below that implements the st stochastic gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    callback(w)\n",
    "    n_samples = model.n_samples\n",
    "    for idx in range(n_iter):\n",
    "        i = idx_samples[idx]\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1e-1\n",
    "callback_sgd = inspector(model, n_iter=n_iter)\n",
    "w_sgd = sgd(model, w0, idx_samples, n_iter=model.n_samples * n_iter, \n",
    "            step=step, callback=callback_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sag'></a>\n",
    "## 3.6. Stochastic average gradient descent\n",
    "\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `sag` below that implements the stochastic averaged gradient algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sag(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic average gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    n_samples, n_features = model.n_samples, model.n_features\n",
    "    gradient_memory = np.zeros((n_samples, n_features))\n",
    "    y = np.zeros(n_features)\n",
    "    callback(w)\n",
    "    for idx in range(n_iter):\n",
    "        i = idx_samples[idx]        \n",
    "        ### TODO\n",
    "\n",
    "        ### END OF TODO        \n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1 / model.lip_max()\n",
    "callback_sag = inspector(model, n_iter=n_iter)\n",
    "w_sag = sag(model, w0, idx_samples, n_iter=model.n_samples * n_iter, \n",
    "            step=step, callback=callback_sag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svrg'></a>\n",
    "## 3.7. Stochastic variance reduced gradient\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "- Finish the function `svrg` below that implements the stochastic variance reduced gradient algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svrg(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic variance reduced gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    w_old = w.copy()\n",
    "    n_samples = model.n_samples\n",
    "    callback(w)\n",
    "    for idx in range(n_iter):        \n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO        \n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1 / model.lip_max()\n",
    "callback_svrg = inspector(model, n_iter=n_iter)\n",
    "w_svrg = svrg(model, w0, idx_samples, n_iter=model.n_samples * n_iter,\n",
    "              step=step, callback=callback_svrg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "# 4. Comparison of all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [callback_gd, callback_agd, callback_cgd, callback_sgd, \n",
    "             callback_sag, callback_svrg]\n",
    "names = [\"GD\", \"AGD\", \"CGD\", \"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "callback_long = inspector(model, n_iter=1000, verbose=False)\n",
    "w_cgd = cgd(model, w0, n_iter=1000, callback=callback_long, verbose=False)\n",
    "obj_min = callback_long.objectives[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "for callback, name in zip(callbacks, names):\n",
    "    objectives = np.array(callback.objectives)\n",
    "    objectives_dist = objectives - obj_min    \n",
    "    plt.plot(objectives_dist, label=name, lw=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim((0, n_iter))\n",
    "plt.xlabel(\"Number of passes on the data\", fontsize=16)\n",
    "plt.ylabel(r\"$F(w^k) - F(w^*)$\", fontsize=16)\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "\n",
    "1. Compare the minimizers you obtain using the different algorithms, with a large and a small number of iterations. This can be done with `plt.stem` plots.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the correlation \n",
    "  of the features on the performance of the optimization algorithms. Explain.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the level of ridge \n",
    "  penalization on the performance of the optimization algorithms. Explain.\n",
    "- (OPTIONAL) All algorithms can be modified to handle an objective of the form $f + g$ with $g$ separable and prox-capable. Modify all the algorithms and try them out for L1 penalization $f(w) = \\lambda \\sum_{j=1}^d |w_j|$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First order methods for regression models\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- gradient descent (GD)\n",
    "- accelerated gradient descent (AGD)\n",
    "- coordinate gradient descent (CD)\n",
    "- stochastic gradient descent (SGD)\n",
    "- stochastic variance reduced gradient descent (SVRG)\n",
    "\n",
    "\n",
    "for the linear regression and logistic regression models, with the \n",
    "ridge penalization.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "## To generate the name of your file, use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp1_veshchezerova_dinara_and_damien_cedric.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using your first and last names\n",
    "fn1 = \"dinara\"\n",
    "ln1 = \"veshchezerova\"\n",
    "fn2 = \"cedric\"\n",
    "ln2 = \"damien\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp1\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "[1. Introduction](#intro)<br>\n",
    "[2. Models gradients and losses](#models)<br>\n",
    "[3. Solvers](#solvers)<br>\n",
    "[4. Comparison of all algorithms](#comparison)<br>\n",
    "\n",
    "<a id='intro'></a>\n",
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Getting model weights\n",
    "\n",
    "We'll start by generating sparse vectors and simulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Simulation of a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(w0, n_samples=1000, corr=0.5, std=0.5):\n",
    "    \"\"\"Simulation of a linear regression model with Gaussian features\n",
    "    and a Toeplitz covariance, with Gaussian noise.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0 : `numpy.array`, shape=(n_features,)\n",
    "        Model weights\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "    \n",
    "    std : `float`, default=0.5\n",
    "        Standard deviation of the noise\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It contains samples of a centered \n",
    "        Gaussian  vector with Toeplitz covariance.\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    n_features = w0.shape[0]\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # Simulation of the labels\n",
    "    y = X.dot(w0) + std * randn(n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f44320>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEgCAYAAACTnoXDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f442b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 50\n",
    "w0 = np.array([0.5])\n",
    "\n",
    "X, y = simu_linreg(w0, n_samples=n_samples, corr=0.3, std=0.5)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(r\"$x_i$\", fontsize=16)\n",
    "plt.ylabel(r\"$y_i$\", fontsize=16)\n",
    "plt.title(\"Linear regression simulation\", fontsize=18)\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Simulation of a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function (overflow-proof)\"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size)    \n",
    "    out[idx] = 1 / (1. + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def simu_logreg(w0, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a logistic regression model with Gaussian features\n",
    "    and a Toeplitz covariance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0 : `numpy.array`, shape=(n_features,)\n",
    "        Model weights\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It contains samples of a centered \n",
    "        Gaussian vector with Toeplitz covariance.\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    n_features = w0.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p = sigmoid(X.dot(w0))\n",
    "    y = np.random.binomial(1, p, size=n_samples)\n",
    "    # Put the label in {-1, 1}\n",
    "    y[:] = 2 * y - 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "w0 = np.array([-3, 3.])\n",
    "\n",
    "X, y = simu_logreg(w0, n_samples=n_samples, corr=0.4)\n",
    "\n",
    "plt.scatter(*X[y == 1].T, color='b', s=10, label=r'$y_i=1$')\n",
    "plt.scatter(*X[y == -1].T, color='r', s=10, label=r'$y_i=-1$')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(r\"$x_i^1$\", fontsize=16)\n",
    "plt.ylabel(r\"$x_i^2$\", fontsize=16)\n",
    "plt.title(\"Logistic regression simulation\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='models'></a>\n",
    "# 2. Models gradients and losses\n",
    "\n",
    "We want to minimize a goodness-of-fit function $f$ with ridge regularization, namely\n",
    "$$\n",
    "\\arg\\min_{w \\in \\mathbb R^d} \\Big\\{ f(w) + \\frac{\\lambda}{2} \\|w\\|_2^2 \\Big\\}\n",
    "$$\n",
    "where $d$ is the number of features and where we will assume that $f$ is $L$-smooth.\n",
    "We will consider below the following cases.\n",
    "\n",
    "**Linear regression**, where \n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - x_i^\\top w)^2 + \\frac{\\lambda}{2} \\|w\\|_2^2 = \\frac{1}{2 n} \\| y - X w \\|_2^2 + \\frac{\\lambda}{2} \\|w\\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, $y = [y_1 \\cdots y_n]$ is the vector of labels and $X$ is the matrix of features with lines containing the features vectors $x_i \\in \\mathbb R^d$.\n",
    "\n",
    "**Logistic regression**, where\n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{n} \\sum_{i=1}^n \\log(1 + \\exp(-y_i x_i^\\top w)) + \\frac{\\lambda}{2} \\|w\\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, and where labels $y_i \\in \\{ -1, 1 \\}$ for all $i$.\n",
    "\n",
    "We need to be able to compute $f(w)$ and its gradient $\\nabla f(w)$, in order to solve this problem, as well as $\\nabla f_i(w)$ for stochastic gradient descent methods and $\\frac{\\partial f(w)}{\\partial w_j}$ for coordinate descent.\n",
    "\n",
    "Below is the full implementation for linear regression.\n",
    "\n",
    "## 2.1 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class ModelLinReg:\n",
    "    \"\"\"A class giving first order information for linear regression\n",
    "    with least-squares loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : `numpy.array`, shape=(n_samples, n_features)\n",
    "        The features matrix\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        The vector of labels\n",
    "    \n",
    "    strength : `float`\n",
    "        The strength of ridge penalization\n",
    "    \"\"\"    \n",
    "    def __init__(self, X, y, strength):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.strength = strength\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        \"\"\"Computes f(w)\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return 0.5 * norm(y - X.dot(w)) ** 2 / n_samples + strength * norm(w) ** 2 / 2\n",
    "    \n",
    "    def grad(self, w):\n",
    "        \"\"\"Computes the gradient of f at w\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return X.T.dot(X.dot(w) - y) / n_samples + strength * w\n",
    "\n",
    "    def grad_i(self, i, w):\n",
    "        \"\"\"Computes the gradient of f_i at w\"\"\"\n",
    "        x_i = self.X[i]\n",
    "        return (x_i.dot(w) - y[i]) * x_i + self.strength * w\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        \"\"\"Computes the partial derivative of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return X[:, j].T.dot(X.dot(w) - y) / n_samples + strength * w[j]\n",
    "\n",
    "    def lip(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return norm(X.T.dot(X), 2) / n_samples + self.strength\n",
    "\n",
    "    def lip_coordinates(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return (X ** 2).sum(axis=0) / n_samples + self.strength\n",
    "        \n",
    "    def lip_max(self):\n",
    "        \"\"\"Computes the maximum of the lipschitz constants of f_i\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return ((X ** 2).sum(axis=1) + self.strength).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Checks for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simulation setting\n",
    "n_features = 50\n",
    "nnz = 20\n",
    "idx = np.arange(n_features)\n",
    "w0 = (-1) ** idx * np.exp(-idx / 10.)\n",
    "w0[nnz:] = 0.\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.stem(w0)\n",
    "plt.title(\"Model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "X, y = simu_linreg(w0, corr=0.6)\n",
    "model = ModelLinReg(X, y, strength=1e-3)\n",
    "w = np.random.randn(n_features)\n",
    "\n",
    "print(check_grad(model.loss, model.grad, w)) # This must be a number (of order 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"lip=\", model.lip())\n",
    "print(\"lip_max=\", model.lip_max())\n",
    "print(\"lip_coordinates=\", model.lip_coordinates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logistic regression\n",
    "\n",
    "**NB**: you can skip these questions and go to the solvers implementation, and come back here later.\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Compute (on paper) the gradient $\\nabla f$, the gradient of $\\nabla f_i$ and the gradient of the coordinate function $\\frac{\\partial f(w)}{\\partial w_j}$ of $f$ for logistic regression (fill the class given below).\n",
    "\n",
    "2. Fill in the functions below for the computation of $f$, $\\nabla f$, $\\nabla f_i$ and $\\frac{\\partial f(w)}{\\partial w_j}$ for logistic regression in the ModelLogReg class below (fill between the TODO and END TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelLogReg:\n",
    "    \"\"\"A class giving first order information for logistic regression\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : `numpy.array`, shape=(n_samples, n_features)\n",
    "        The features matrix\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        The vector of labels\n",
    "    \n",
    "    strength : `float`\n",
    "        The strength of ridge penalization\n",
    "    \"\"\"    \n",
    "    def __init__(self, X, y, strength):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.strength = strength\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        \"\"\"Computes f(w)\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "       \n",
    "    def grad(self, w):\n",
    "        \"\"\"Computes the gradient of f at w\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def grad_i(self, i, w):\n",
    "        \"\"\"Computes the gradient of f_i at w\"\"\"\n",
    "        x_i = self.X[i], strength = self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        \"\"\"Computes the partial derivative of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip_coordinates(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip_max(self):\n",
    "        \"\"\"Computes the maximum of the lipschitz constants of f_i\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Checks for the logistic regression model\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Check numerically the gradient using the function ``checkgrad`` from ``scipy.optimize`` (see below), as we did for linear regression above\n",
    "\n",
    "**Remark**: use the function `simu_logreg` to simulate data according to the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='solvers'></a>\n",
    "## 3. Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have classes `ModelLinReg` and `ModelLogReg` that allow to compute $f(w)$, $\\nabla f(w)$, \n",
    "$\\nabla f_i(w)$ and $\\frac{\\partial f(w)}{\\partial w_j}$ for the objective $f$\n",
    "given by linear and logistic regression.\n",
    "\n",
    "We want now to code and compare several solvers to minimize $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tools'></a>\n",
    "## 3.1. Tools for the solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting point of all solvers\n",
    "w0 = np.zeros(model.n_features)\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 50\n",
    "\n",
    "# Random samples indices for the stochastic solvers (sgd, sag, svrg)\n",
    "idx_samples = np.random.randint(0, model.n_samples, model.n_samples * n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspector(model, n_iter, verbose=True):\n",
    "    \"\"\"A closure called to update metrics after each iteration.\n",
    "    Don't even look at it, we'll just use it in the solvers.\"\"\"\n",
    "    objectives = []\n",
    "    it = [0] # This is a hack to be able to modify 'it' inside the closure.\n",
    "    def inspector_cl(w):\n",
    "        obj = model.loss(w)\n",
    "        objectives.append(obj)\n",
    "        if verbose == True:\n",
    "            if it[0] == 0:\n",
    "                print(' | '.join([name.center(8) for name in [\"it\", \"obj\"]]))\n",
    "            if it[0] % (n_iter / 5) == 0:\n",
    "                print(' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8)]))\n",
    "            it[0] += 1\n",
    "    inspector_cl.objectives = objectives\n",
    "    return inspector_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gd'></a>\n",
    "## 3.2 Gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `gd` below that implements the gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Gradient descent\n",
    "    \"\"\"\n",
    "    step = 1 / model.lip()\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    if verbose:\n",
    "        print(\"Lauching GD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_gd = inspector(model, n_iter=n_iter)\n",
    "w_gd = gd(model, w0, n_iter=n_iter, callback=callback_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='agd'></a>\n",
    "## 3.3 Accelerated gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `agd` below that implements the accelerated gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Accelerated gradient descent\n",
    "    \"\"\"\n",
    "    step = 1 / model.lip()\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    # An extra variable is required for acceleration\n",
    "    z = w0.copy()\n",
    "    t = 1.\n",
    "    t_new = 1.    \n",
    "    if verbose:\n",
    "        print(\"Lauching AGD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO        \n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_agd = inspector(model, n_iter=n_iter)\n",
    "w_agd = agd(model, w0, n_iter=n_iter, callback=callback_agd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cgd'></a>\n",
    "\n",
    "## 3.4 Coordinate gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `cgd` below that implements the coordinate gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cgd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Coordinate gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    n_features = model.n_features\n",
    "    steps = 1 / model.lip_coordinates()\n",
    "    if verbose:\n",
    "        print(\"Lauching CGD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_cgd = inspector(model, n_iter=n_iter)\n",
    "w_cgd = cgd(model, w0, n_iter=n_iter, callback=callback_cgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>\n",
    "## 3.5. Stochastic gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "- Finish the function `sgd` below that implements the st stochastic gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    callback(w)\n",
    "    n_samples = model.n_samples\n",
    "    for idx in range(n_iter):\n",
    "        i = idx_samples[idx]\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1e-1\n",
    "callback_sgd = inspector(model, n_iter=n_iter)\n",
    "w_sgd = sgd(model, w0, idx_samples, n_iter=model.n_samples * n_iter, \n",
    "            step=step, callback=callback_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sag'></a>\n",
    "## 3.6. Stochastic average gradient descent\n",
    "\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `sag` below that implements the stochastic averaged gradient algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sag(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic average gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    n_samples, n_features = model.n_samples, model.n_features\n",
    "    gradient_memory = np.zeros((n_samples, n_features))\n",
    "    y = np.zeros(n_features)\n",
    "    callback(w)\n",
    "    for idx in range(n_iter):\n",
    "        i = idx_samples[idx]        \n",
    "        ### TODO\n",
    "\n",
    "        ### END OF TODO        \n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1 / model.lip_max()\n",
    "callback_sag = inspector(model, n_iter=n_iter)\n",
    "w_sag = sag(model, w0, idx_samples, n_iter=model.n_samples * n_iter, \n",
    "            step=step, callback=callback_sag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svrg'></a>\n",
    "## 3.7. Stochastic variance reduced gradient\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "- Finish the function `svrg` below that implements the stochastic variance reduced gradient algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svrg(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic variance reduced gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    w_old = w.copy()\n",
    "    n_samples = model.n_samples\n",
    "    callback(w)\n",
    "    for idx in range(n_iter):        \n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO        \n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1 / model.lip_max()\n",
    "callback_svrg = inspector(model, n_iter=n_iter)\n",
    "w_svrg = svrg(model, w0, idx_samples, n_iter=model.n_samples * n_iter,\n",
    "              step=step, callback=callback_svrg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "# 4. Comparison of all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [callback_gd, callback_agd, callback_cgd, callback_sgd, \n",
    "             callback_sag, callback_svrg]\n",
    "names = [\"GD\", \"AGD\", \"CGD\", \"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "callback_long = inspector(model, n_iter=1000, verbose=False)\n",
    "w_cgd = cgd(model, w0, n_iter=1000, callback=callback_long, verbose=False)\n",
    "obj_min = callback_long.objectives[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "for callback, name in zip(callbacks, names):\n",
    "    objectives = np.array(callback.objectives)\n",
    "    objectives_dist = objectives - obj_min    \n",
    "    plt.plot(objectives_dist, label=name, lw=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim((0, n_iter))\n",
    "plt.xlabel(\"Number of passes on the data\", fontsize=16)\n",
    "plt.ylabel(r\"$F(w^k) - F(w^*)$\", fontsize=16)\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "\n",
    "1. Compare the minimizers you obtain using the different algorithms, with a large and a small number of iterations. This can be done with `plt.stem` plots.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the correlation \n",
    "  of the features on the performance of the optimization algorithms. Explain.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the level of ridge \n",
    "  penalization on the performance of the optimization algorithms. Explain.\n",
    "- (OPTIONAL) All algorithms can be modified to handle an objective of the form $f + g$ with $g$ separable and prox-capable. Modify all the algorithms and try them out for L1 penalization $f(w) = \\lambda \\sum_{j=1}^d |w_j|$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

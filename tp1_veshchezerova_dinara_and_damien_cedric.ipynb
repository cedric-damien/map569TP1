{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First order methods for regression models\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- gradient descent (GD)\n",
    "- accelerated gradient descent (AGD)\n",
    "- coordinate gradient descent (CD)\n",
    "- stochastic gradient descent (SGD)\n",
    "- stochastic variance reduced gradient descent (SVRG)\n",
    "\n",
    "\n",
    "for the linear regression and logistic regression models, with the \n",
    "ridge penalization.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "## To generate the name of your file, use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp1_veshchezerova_dinara_and_damien_cedric.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using your first and last names\n",
    "fn1 = \"dinara\"\n",
    "ln1 = \"veshchezerova\"\n",
    "fn2 = \"cedric\"\n",
    "ln2 = \"damien\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp1\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "[1. Introduction](#intro)<br>\n",
    "[2. Models gradients and losses](#models)<br>\n",
    "[3. Solvers](#solvers)<br>\n",
    "[4. Comparison of all algorithms](#comparison)<br>\n",
    "\n",
    "<a id='intro'></a>\n",
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Getting model weights\n",
    "\n",
    "We'll start by generating sparse vectors and simulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Simulation of a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(w0, n_samples=1000, corr=0.5, std=0.5):\n",
    "    \"\"\"Simulation of a linear regression model with Gaussian features\n",
    "    and a Toeplitz covariance, with Gaussian noise.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0 : `numpy.array`, shape=(n_features,)\n",
    "        Model weights\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "    \n",
    "    std : `float`, default=0.5\n",
    "        Standard deviation of the noise\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It contains samples of a centered \n",
    "        Gaussian  vector with Toeplitz covariance.\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    n_features = w0.shape[0]\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # Simulation of the labels\n",
    "    y = X.dot(w0) + std * randn(n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10a98bfd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEgCAYAAACq+TSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x60d2a3110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 500\n",
    "w0 = np.array([0.5])\n",
    "\n",
    "X, y = simu_linreg(w0, n_samples=n_samples, corr=0.3, std=0.5)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(r\"$x_i$\", fontsize=16)\n",
    "plt.ylabel(r\"$y_i$\", fontsize=16)\n",
    "plt.title(\"Linear regression simulation\", fontsize=18)\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Simulation of a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function (overflow-proof)\"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size)    \n",
    "    out[idx] = 1 / (1. + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def simu_logreg(w0, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a logistic regression model with Gaussian features\n",
    "    and a Toeplitz covariance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0 : `numpy.array`, shape=(n_features,)\n",
    "        Model weights\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It contains samples of a centered \n",
    "        Gaussian vector with Toeplitz covariance.\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    n_features = w0.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p = sigmoid(X.dot(w0))\n",
    "    y = np.random.binomial(1, p, size=n_samples)\n",
    "    # Put the label in {-1, 1}\n",
    "    y[:] = 2 * y - 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "w0 = np.array([-3, 3.])\n",
    "\n",
    "X, y = simu_logreg(w0, n_samples=n_samples, corr=0.4)\n",
    "\n",
    "plt.scatter(*X[y == 1].T, color='b', s=10, label=r'$y_i=1$')\n",
    "plt.scatter(*X[y == -1].T, color='r', s=10, label=r'$y_i=-1$')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(r\"$x_i^1$\", fontsize=16)\n",
    "plt.ylabel(r\"$x_i^2$\", fontsize=16)\n",
    "plt.title(\"Logistic regression simulation\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='models'></a>\n",
    "# 2. Models gradients and losses\n",
    "\n",
    "We want to minimize a goodness-of-fit function $f$ with ridge regularization, namely\n",
    "$$\n",
    "\\arg\\min_{w \\in \\mathbb R^d} \\Big\\{ f(w) + \\frac{\\lambda}{2} \\|w\\|_2^2 \\Big\\}\n",
    "$$\n",
    "where $d$ is the number of features and where we will assume that $f$ is $L$-smooth.\n",
    "We will consider below the following cases.\n",
    "\n",
    "**Linear regression**, where \n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - x_i^\\top w)^2 + \\frac{\\lambda}{2} \\|w\\|_2^2 = \\frac{1}{2 n} \\| y - X w \\|_2^2 + \\frac{\\lambda}{2} \\|w\\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, $y = [y_1 \\cdots y_n]$ is the vector of labels and $X$ is the matrix of features with lines containing the features vectors $x_i \\in \\mathbb R^d$.\n",
    "\n",
    "**Logistic regression**, where\n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{n} \\sum_{i=1}^n \\log(1 + \\exp(-y_i x_i^\\top w)) + \\frac{\\lambda}{2} \\|w\\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, and where labels $y_i \\in \\{ -1, 1 \\}$ for all $i$.\n",
    "\n",
    "We need to be able to compute $f(w)$ and its gradient $\\nabla f(w)$, in order to solve this problem, as well as $\\nabla f_i(w)$ for stochastic gradient descent methods and $\\frac{\\partial f(w)}{\\partial w_j}$ for coordinate descent.\n",
    "\n",
    "Below is the full implementation for linear regression.\n",
    "\n",
    "## 2.1 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class ModelLinReg:\n",
    "    \"\"\"A class giving first order information for linear regression\n",
    "    with least-squares loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : `numpy.array`, shape=(n_samples, n_features)\n",
    "        The features matrix\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        The vector of labels\n",
    "    \n",
    "    strength : `float`\n",
    "        The strength of ridge penalization\n",
    "    \"\"\"    \n",
    "    def __init__(self, X, y, strength):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.strength = strength\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        \"\"\"Computes f(w)\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return 0.5 * norm(y - X.dot(w)) ** 2 / n_samples + strength * norm(w) ** 2 / 2\n",
    "    \n",
    "    def grad(self, w):\n",
    "        \"\"\"Computes the gradient of f at w\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return X.T.dot(X.dot(w) - y) / n_samples + strength * w\n",
    "\n",
    "    def grad_i(self, i, w):\n",
    "        \"\"\"Computes the gradient of f_i at w\"\"\"\n",
    "        x_i = self.X[i]\n",
    "        return (x_i.dot(w) - y[i]) * x_i + self.strength * w\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        \"\"\"Computes the partial derivative of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return X[:, j].T.dot(X.dot(w) - y) / n_samples + strength * w[j]\n",
    "\n",
    "    def lip(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return norm(X.T.dot(X), 2) / n_samples + self.strength\n",
    "\n",
    "    def lip_coordinates(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return (X ** 2).sum(axis=0) / n_samples + self.strength\n",
    "        \n",
    "    def lip_max(self):\n",
    "        \"\"\"Computes the maximum of the lipschitz constants of f_i\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return ((X ** 2).sum(axis=1) + self.strength).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Checks for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simulation setting\n",
    "n_features = 50\n",
    "nnz = 20\n",
    "idx = np.arange(n_features)\n",
    "w0 = (-1) ** idx * np.exp(-idx / 10.)\n",
    "w0[nnz:] = 0.\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.stem(w0)\n",
    "plt.title(\"Model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "X, y = simu_linreg(w0, corr=0.6)\n",
    "model = ModelLinReg(X, y, strength=1e-3)\n",
    "w = np.random.randn(n_features)\n",
    "\n",
    "print(check_grad(model.loss, model.grad, w)) # This must be a number (of order 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"lip=\", model.lip())\n",
    "print(\"lip_max=\", model.lip_max())\n",
    "print(\"lip_coordinates=\", model.lip_coordinates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logistic regression\n",
    "\n",
    "**NB**: you can skip these questions and go to the solvers implementation, and come back here later.\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Compute (on paper) the gradient $\\nabla f$, the gradient of $\\nabla f_i$ and the gradient of the coordinate function $\\frac{\\partial f(w)}{\\partial w_j}$ of $f$ for logistic regression (fill the class given below).\n",
    "\n",
    "2. Fill in the functions below for the computation of $f$, $\\nabla f$, $\\nabla f_i$ and $\\frac{\\partial f(w)}{\\partial w_j}$ for logistic regression in the ModelLogReg class below (fill between the TODO and END TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelLogReg:\n",
    "    \"\"\"A class giving first order information for logistic regression\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : `numpy.array`, shape=(n_samples, n_features)\n",
    "        The features matrix\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        The vector of labels\n",
    "    \n",
    "    strength : `float`\n",
    "        The strength of ridge penalization\n",
    "    \"\"\"    \n",
    "    def __init__(self, X, y, strength):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.strength = strength\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        \"\"\"Computes f(w)\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "       \n",
    "    def grad(self, w):\n",
    "        \"\"\"Computes the gradient of f at w\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def grad_i(self, i, w):\n",
    "        \"\"\"Computes the gradient of f_i at w\"\"\"\n",
    "        x_i = self.X[i], strength = self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        \"\"\"Computes the partial derivative of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip_coordinates(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip_max(self):\n",
    "        \"\"\"Computes the maximum of the lipschitz constants of f_i\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Checks for the logistic regression model\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Check numerically the gradient using the function ``checkgrad`` from ``scipy.optimize`` (see below), as we did for linear regression above\n",
    "\n",
    "**Remark**: use the function `simu_logreg` to simulate data according to the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='solvers'></a>\n",
    "## 3. Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have classes `ModelLinReg` and `ModelLogReg` that allow to compute $f(w)$, $\\nabla f(w)$, \n",
    "$\\nabla f_i(w)$ and $\\frac{\\partial f(w)}{\\partial w_j}$ for the objective $f$\n",
    "given by linear and logistic regression.\n",
    "\n",
    "We want now to code and compare several solvers to minimize $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tools'></a>\n",
    "## 3.1. Tools for the solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting point of all solvers\n",
    "w0 = np.zeros(model.n_features)\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 50\n",
    "\n",
    "# Random samples indices for the stochastic solvers (sgd, sag, svrg)\n",
    "idx_samples = np.random.randint(0, model.n_samples, model.n_samples * n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspector(model, n_iter, verbose=True):\n",
    "    \"\"\"A closure called to update metrics after each iteration.\n",
    "    Don't even look at it, we'll just use it in the solvers.\"\"\"\n",
    "    objectives = []\n",
    "    it = [0] # This is a hack to be able to modify 'it' inside the closure.\n",
    "    def inspector_cl(w):\n",
    "        obj = model.loss(w)\n",
    "        objectives.append(obj)\n",
    "        if verbose == True:\n",
    "            if it[0] == 0:\n",
    "                print(' | '.join([name.center(8) for name in [\"it\", \"obj\"]]))\n",
    "            if it[0] % (n_iter / 5) == 0:\n",
    "                print(' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8)]))\n",
    "            it[0] += 1\n",
    "    inspector_cl.objectives = objectives\n",
    "    return inspector_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gd'></a>\n",
    "## 3.2 Gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `gd` below that implements the gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Gradient descent\n",
    "    \"\"\"\n",
    "    step = 1 / model.lip()\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    if verbose:\n",
    "        print(\"Lauching GD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_gd = inspector(model, n_iter=n_iter)\n",
    "w_gd = gd(model, w0, n_iter=n_iter, callback=callback_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='agd'></a>\n",
    "## 3.3 Accelerated gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `agd` below that implements the accelerated gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Accelerated gradient descent\n",
    "    \"\"\"\n",
    "    step = 1 / model.lip()\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    # An extra variable is required for acceleration\n",
    "    z = w0.copy()\n",
    "    t = 1.\n",
    "    t_new = 1.    \n",
    "    if verbose:\n",
    "        print(\"Lauching AGD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO        \n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_agd = inspector(model, n_iter=n_iter)\n",
    "w_agd = agd(model, w0, n_iter=n_iter, callback=callback_agd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cgd'></a>\n",
    "\n",
    "## 3.4 Coordinate gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `cgd` below that implements the coordinate gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cgd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Coordinate gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    n_features = model.n_features\n",
    "    steps = 1 / model.lip_coordinates()\n",
    "    if verbose:\n",
    "        print(\"Lauching CGD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_cgd = inspector(model, n_iter=n_iter)\n",
    "w_cgd = cgd(model, w0, n_iter=n_iter, callback=callback_cgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>\n",
    "## 3.5. Stochastic gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "- Finish the function `sgd` below that implements the st stochastic gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    callback(w)\n",
    "    n_samples = model.n_samples\n",
    "    for idx in range(n_iter):\n",
    "        i = idx_samples[idx]\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1e-1\n",
    "callback_sgd = inspector(model, n_iter=n_iter)\n",
    "w_sgd = sgd(model, w0, idx_samples, n_iter=model.n_samples * n_iter, \n",
    "            step=step, callback=callback_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sag'></a>\n",
    "## 3.6. Stochastic average gradient descent\n",
    "\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `sag` below that implements the stochastic averaged gradient algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sag(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic average gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    n_samples, n_features = model.n_samples, model.n_features\n",
    "    gradient_memory = np.zeros((n_samples, n_features))\n",
    "    y = np.zeros(n_features)\n",
    "    callback(w)\n",
    "    for idx in range(n_iter):\n",
    "        i = idx_samples[idx]        \n",
    "        ### TODO\n",
    "\n",
    "        ### END OF TODO        \n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1 / model.lip_max()\n",
    "callback_sag = inspector(model, n_iter=n_iter)\n",
    "w_sag = sag(model, w0, idx_samples, n_iter=model.n_samples * n_iter, \n",
    "            step=step, callback=callback_sag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svrg'></a>\n",
    "## 3.7. Stochastic variance reduced gradient\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "- Finish the function `svrg` below that implements the stochastic variance reduced gradient algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svrg(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic variance reduced gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    w_old = w.copy()\n",
    "    n_samples = model.n_samples\n",
    "    callback(w)\n",
    "    for idx in range(n_iter):        \n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO        \n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1 / model.lip_max()\n",
    "callback_svrg = inspector(model, n_iter=n_iter)\n",
    "w_svrg = svrg(model, w0, idx_samples, n_iter=model.n_samples * n_iter,\n",
    "              step=step, callback=callback_svrg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "# 4. Comparison of all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [callback_gd, callback_agd, callback_cgd, callback_sgd, \n",
    "             callback_sag, callback_svrg]\n",
    "names = [\"GD\", \"AGD\", \"CGD\", \"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "callback_long = inspector(model, n_iter=1000, verbose=False)\n",
    "w_cgd = cgd(model, w0, n_iter=1000, callback=callback_long, verbose=False)\n",
    "obj_min = callback_long.objectives[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "for callback, name in zip(callbacks, names):\n",
    "    objectives = np.array(callback.objectives)\n",
    "    objectives_dist = objectives - obj_min    \n",
    "    plt.plot(objectives_dist, label=name, lw=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim((0, n_iter))\n",
    "plt.xlabel(\"Number of passes on the data\", fontsize=16)\n",
    "plt.ylabel(r\"$F(w^k) - F(w^*)$\", fontsize=16)\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "\n",
    "1. Compare the minimizers you obtain using the different algorithms, with a large and a small number of iterations. This can be done with `plt.stem` plots.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the correlation \n",
    "  of the features on the performance of the optimization algorithms. Explain.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the level of ridge \n",
    "  penalization on the performance of the optimization algorithms. Explain.\n",
    "- (OPTIONAL) All algorithms can be modified to handle an objective of the form $f + g$ with $g$ separable and prox-capable. Modify all the algorithms and try them out for L1 penalization $f(w) = \\lambda \\sum_{j=1}^d |w_j|$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
